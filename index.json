[{"authors":["admin"],"categories":null,"content":"I am an associate professor in the School of Computing (SoC) at KAIST and a faculty member of Computer Architecture and Systems Laboratory (CASYS). I am co-affiliated with: School of Electrical Engineering Graduate School of AI Semiconductor Graduate School of System Architect Kim Jaechul Graduate School of AI Department of Semiconductor System Engineering I am very much looking forward to working with talented students on exciting projects. If you're interested in joining my research group, please contact me via email introducing yourself (with your CV and transcripts if available).\nI am currently on sabbatical, visiting the Pervasive Parallelism Lab at Stanford University. ","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://jongse-park.github.io/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"I am an associate professor in the School of Computing (SoC) at KAIST and a faculty member of Computer Architecture and Systems Laboratory (CASYS). I am co-affiliated with: School of Electrical Engineering Graduate School of AI Semiconductor Graduate School of System Architect Kim Jaechul Graduate School of AI Department of Semiconductor System Engineering I am very much looking forward to working with talented students on exciting projects. If you're interested in joining my research group, please contact me via email introducing yourself (with your CV and transcripts if available).","tags":null,"title":"Jongse Park","type":"authors"},{"authors":null,"categories":null,"content":"2025 Pimba: A Processing-in-Memory Acceleration for Post-Transformer Large Language Model Serving\nWonung Kim, Yubin Lee, Yoonsung Kim, Jinwoo Hwang, Seongryong Oh, Jiyong Jung, Aziz Huseynov, Woong Gyu Park, Chang Hyun Park, Divya Mahajan, Jongse Park\nMICRO, 2025 [Paper] (To Appear)\nPyTorchSim: A Comprehensive, Fast, and Accurate NPU Simulation Framework\nWonhyuk Yang, Yunseon Shin, Okkyun Woo, Geonwoo Park, Hyungkyu Ham, Jeehoon Kang, Jongse Park, Gwangsun Kim\nMICRO, 2025 [Paper] (To Appear)\nDéjà Vu: Efficient Video-Language Query Engine with Learning-based Inter-Frame Computation Reuse\nJinwoo Hwang, Daeun Kim, Sangyeop Lee, Yoonsung Kim, Guseul Heo, Hojoon Kim, Yunseok Jeong, Tadiwos Meaza, Eunhyeok Park, Jeongseob Ahn, Jongse Park\nVLDB, 2025 [Paper] (To Appear)\nOaken: Fast and Efficient LLM Serving with Online-Offline Hybrid KV Cache Quantization\nMinsu Kim, Seongmin Hong, RyeoWook Ko, Soongyu Choi, Hunjong Lee, Junsoo Kim, Joo-Young Kim, Jongse Park\nISCA, 2025 [Paper|Talk]\nMixDiT: Accelerating Image Diffusion Transformer Inference with Mixed-Precision MX Quantization\nDaeun Kim, Jinwoo Hwang, Changhun Oh, Jongse Park\nIEEE Computer Architecture Letters (CAL), 2025 [Paper]\n2024 Interference-Aware DNN Serving on Heterogeneous Processors in Edge Systems\nYeonjae Kim, Igjae Kim, Kwanghoon Choi, Jeongseob Ahn, Jongse Park, Jaehyuk Huh\nICCD, 2024 [Paper]\nLLMServingSim: A HW/SW Co-Simulation Infrastructure for LLM Inference Serving at Scale\nJaehong Cho, Minsu Kim, Hyunmin Choi, Guseul Heo, Jongse Park\nIISWC, 2024 [Paper|Talk|Code] Best Paper Award \u0026amp; Distinguished Artifact Award Accelerating String-key Learned Index Structures via Memoization-based Incremental Training\nMinsu Kim, Jinwoo Hwang, Guseul Heo, Seiyeon Cho, Divya Mahajan, Jongse Park\nVLDB, 2024 [Paper|Talk|Code]\nDaCapo: Accelerating Continuous Learning in Autonomous Systems for Video Analytics\nYoonsung Kim, Changhun Oh, Jinwoo Hwang, Wonung Kim, Seongryong Oh, Yubin Lee, Hardik Sharma, Amir Yazdanbakhsh, Jongse Park\nISCA, 2024 [Paper|Talk|Code]\nDistinguished Artifact Award\nLLMServingSim: A Simulation Infrastructure for LLM Inference Serving Systems\nJaehong Cho, Minsu Kim, Hyunmin Choi, Jongse Park\nISCA Workshop on ML for Computer Architecture and Systems (MLArchSys), 2024 [Paper|Talk]\nLVS: A Learned Video Storage for Fast and Efficient Video Understanding\nYunghee Lee, Jongse Park\nCVPR Workshop on Efficient Deep Learning for Computer Vision (ECV), 2024 [Paper|Talk]\nNeuPIMs: NPU-PIM Heterogeneous Acceleration for Batched LLM Inferencing\nGuseul Heo, Sangyeop Lee, Jaehong Cho, Hyunmin Choi, Sanghyeon Lee, Hyungkyu Ham, Gwangsun Kim, Divya Mahajan, Jongse Park\nASPLOS, 2024 [Paper|Talk|Code]\nTandem Processor: Grappling with Emerging Operators in Neural Networks\nSoroush Ghodrati, Sean Kinzer, Hanyang Xu, Rohan Mahapatra, Yoonsung Kim, Byung Hoon Ahn, Dong Kai Wang, Lavanya Karthikeyan, Amir Yazdanbakhsh, Jongse Park, Nam Sung Kim, Hadi Esmaeilzadeh\nASPLOS, 2024 [Paper] Honorable Mention in IEEE Micro Top Picks\nONNXim: A Fast, Cycle-level Multi-core NPU Simulator\nHyungkyu Ham*, Wonhyuk Yang*, Yunseon Shin, Okkyun Woo, Guseul Heo, Sangyeop Lee, Jongse Park, Gwangsun Kim\nIEEE Computer Architecture Letters (CAL), 2024 [Paper|Code]\nLPU: A Latency-optimized and Highly Scalable Processor for Large Language Model Inference\nSeungjae Moon, Jung-Hoon Kim, Junsoo Kim, Seongmin Hong, Junseo Cha, Minsu Kim, Sukbin Lim, Gyubin Choi, Dongjin Seo, Jongho Kim, Hunjong Lee, Hyunjun Park, Ryeowook Ko, Soongyu Choi, Jongse Park, Jinwon Lee, Joo-Young Kim\nIEEE Micro, special issue on Contemporary Industry Products, 2024 [Paper] IEEE Mirco Best Paper Award 2024 Cerberus: Triple Mode Acceleration of Sparse Matrix and Vector Multiplication\nSoojin Hwang, Daehyeon Baek, Jongse Park, Jaehyuk Huh\nIEEE Transactions on Architecture and Code Optimization (TACO), 2024 [Paper]\n2023 Bit Fusion: Bit-Level Dynamically Composable Architecture for Accelerating Deep Neural Network\nHardik Sharma, Jongse Park, Naveen Suda, Liangzhen Lai, Benson Chau, Vikas Chandra, Hadi Esmaeilzadeh\nISCA, 2023 [Retrospective]\nSelected for Inclusion in ISCA 25-year Retrospective 1996-2020\nGeneral-Purpose Code Acceleration with Limited-Precision Analog Computation\nRenée St. Amant, Amir Yazdanbakhsh, Jongse Park, Bradley Thwaites, Hadi Esmaeilzadeh, Arjang Hassibi, Luis Ceze, and Doug Burger\nISCA, 2023 [Retrospective]\nSelected for Inclusion in ISCA 25-year Retrospective 1996-2020\nHardware Hardened Sandbox Enclaves for Trusted Serverless Computing\nJoongun Park, Seunghyo Kang, Sanghyeon Lee, Taehoon Kim, Jongse Park, Youngjin Kwon, and Jaehyuk Huh\nIEEE Transactions on Architecture and Code Optimization (TACO), 2023 [Paper]\nFlexBlock: A Flexible DNN Training Accelerator with Multi-Mode Block Floating Point Support\nSeock-Hwan Noh, Jahyun Koo, Seunghyun Lee, Jongse Park, and Jaeha Kung\nIEEE Transactions on Computers (TC), 2023 [Paper]\nHAMMER: Hardware-friendly Approximate Computing for Self-attention with Mean-redistribution and Linearization\nSeonho Lee, Ranggi Hwang, Jongse Park, and Minsoo Rhu\nIEEE Computer Architecture Letters (CAL), 2023 [Paper]\n2022 Tunable Memory Protection for Secure Neural Processing Units\nSunho Lee, Seonjin Na, Jungwoo Kim, Jongse Park, and Jaehyuk Huh\nICCD, 2022 [Paper]\nSupporting Dynamic Translation Granularity for Hybrid Memory Systems\nBokyeong Kim, Soojin Hwang, Sanghoon Cha, Chang Hyun Park, Jongse Park, and Jaehyuk Huh\nICCD, 2022 [Paper]\nCoVA: Exploiting Compressed-Domain Analysis to Accelerate Video Analytics\nJinwoo Hwang, Minsu Kim, Daeun Kim, Seungho Nam, Yoonsung Kim, Dohee Kim, Hardik Sharma, Jongse Park\nUSENIX ATC, 2022 [Paper|Talk]\nServing Heterogeneous Machine Learning Models on Multi-GPU Servers with Spatio-Temporal Sharing\nSeungbeom Choi, Sunho Lee, Yeonjae Kim, Jongse Park, Youngjin Kwon, and Jaehyuk Huh\nUSENIX ATC, 2022 [Paper|Talk]\nTNPU: Supporting Trusted Execution with Tree-less Integrity Protection for Neural Processing Unit\nSunho Lee, Jungwoo Kim, Seonjin Na, Jongse Park, and Jaehyuk Huh\nHPCA, 2022 [Paper|Talk]\nYin-Yang: Programming Abstraction for Cross-Domain Multi-Acceleration\nJoon Kyung Kim, Byung Hoon Ahn, Sean Kinzer, Soroush Ghodrati, Rohan Mahapatra, Brahmendra Yatham, Dohee Kim, Parisa Sarikhani, Babak Mahmoudi, Divya Mahajan, Jongse Park, Hadi Esmaeilzadeh\nIEEE Micro, special issue on Compiling for Accelerators, 2022 [Paper]\n2021 Common Counters: Compressed Encryption Counters for Secure GPU Memory\nSeonjin Na, Sunho Lee, Yeonjae Kim, Jongse Park, and Jaehyuk Huh\nHPCA, 2021 [Paper|Talk]\nSLO-aware Inference Scheduler for Heterogeneous Processors in Edge Platforms\nWonik Seo, Sanghoon Cha, Yeonjae Kim, Jaehyuk Huh, and Jongse Park\nIEEE Transactions on Architecture and Code Optimization (TACO), 2021 [Paper]\n2020 Mixed-Signal Charge-Domain Acceleration of Deep Neural Networks through Interleaved Bit-Partitioned Arithmetic Soroush Ghodrati, Hardik Sharma, Sean Kinzer, Amir Yazdanbakhsh, Jongse Park, Nam Sung Kim, Doug Burger, and Hadi Esmaeilzadeh\nPACT, 2020 [Paper|Talk]\n2018 or earlier A Network-Centric Hardware/Algorithm Co-Design to Accelerate Distributed Training of Deep Neural Networks Youjie Li, Jongse Park, Mohammad Alian, Yifan Yuan, Zheng Qu, Peitian Pan, Ren Wang, Alexander Gerhard Schwing, Hadi Esmaeilzadeh, and Nam Sung Kim\nMICRO, 2018 [Paper|Talk]\nFrom Tensors to FPGAs: Accelerating Deep Learning\nHardik Sharma, Jongse Park, Balavinayagam Samynathan, Behnam Robatmili, Shahrzad Mirkhani, and Hadi Esmaeilzadeh\nHotChips, 2018 [Paper|Poster|Demo1|Demo2]\nBit Fusion: Bit-Level Dynamically Composable Architecture for Accelerating Deep Neural Networks\nHardik Sharma, Jongse Park, Naveen Suda, Liangzhen Lai, Benson Chau, Vikas Chandra, Hadi Esmaeilzadeh\nISCA, 2018 [Paper|Talk]\nScale-Out Acceleration for Machine Learning\nJongse Park, Hardik Sharma, Divya Mahajan, Joon Kyung Kim, Preston Olds, and Hadi Esmaeilzadeh\nMICRO, 2017 [Paper|Talk]\nAxGames: Towards Crowdsourcing Quality Target Determination in Approximate Computing\nJongse Park, Divya Mahajan, Bradley Thwaites, Emmanuel Amaro, and Hadi Esmaeilzadeh ASPLOS, 2016 [Paper|Talk]\nFrom High-Level Deep Neural Models to FPGAs\nHardik Sharma, Jongse Park, Divya Mahajan, Emmanuel Amaro, Joon Kyung Kim, Chenkai Shao, Asit Mishra, and Hadi Esmaeilzadeh\nMICRO, 2016 [Paper|Talk]\nTowards Statistical Guarantees in Controlling Quality Tradeoffs in Approximate Acceleration\nDivya Mahajan, Amir Yazdanbaksh, Jongse Park, Bradley Thwaites, and Hadi Esmaeilzadeh\nISCA, 2016 [Paper|Talk]\nTabla: A Unified Template-based Framework for Accelerating Statistical Machine Learning\nDivya Mahajan, Jongse Park, Emmanuel Amaro, Hardik Sharma, Amir Yazdanbaksh, Joon Kyung Kim, and Hadi Esmaeilzadeh\nHPCA, 2016 [Paper|Talk]\nDistinguished Paper Award\nFlexJava: Language Support for Safe and Modular Approximate Programming\nJongse Park, Hadi Esmaeilzadeh, Xin Zhang, Mayur Naik, William Harris\nFSE, 2015 [Paper|Talk|Artifact]\nNeural Acceleration for GPU Throughput Processors\nAmir Yazdanbakhsh, Jongse Park, Hardik Sharma, Pejman Lofti-Kamran, and Hadi Esmaeilzadeh\nMICRO, 2015 [Paper|Talk]\nAxilog: Language Support for Approximate Hardware Design\nAmir Yazdanbakhsh, Divya Mahajan, Bradley Thwaites, Jongse Park, Anandhavel Nagendrakumar, Sindhuja Sethuraman, Kartik Ramkrishnan, Nishanthi Ravindran, Rudra Jariwala, Abbas Rahimi, Hadi Esmaeilzadeh, and Kia Bazargan\nDATE, 2015 [Paper|Talk]\nAxilog: Abstractions for Approximate Hardware Design and Reuse\nDivya Mahajan, Kartik Ramkrishnan, Rudra Jariwala, Amir Yazdanbakhsh, Jongse Park, Bradley Thwaites, Anandhavel Nagendrakumar, Abbas Rahimi, Hadi Esmaeilzadeh, and Kia Bazargan\nIEEE Micro, , special issue on Alternative Computing Designs and Technologies, 2015 [Paper]\nGeneral-Purpose Code Acceleration with Limited-Precision Analog Computation\nRenée St. Amant, Amir Yazdanbakhsh, Jongse Park, Bradley Thwaites, Hadi Esmaeilzadeh, Arjang Hassibi, Luis Ceze, and Doug Burger\nISCA, 2014 [Paper|Talk]\nHonorable Mention in IEEE Micro Top Picks\nRollbak-Free Value Prediction with Approximate Loads (Short paper)\nBradley Thwaites, Gennady Pekhimenko, Amir Yazdanbakhsh, Jongse Park, Girish Mururu, Hadi Esmaeilzadeh, Onur Mutlu, and Todd Mowry\nPACT, 2014 [Paper]\nIsolated Mini-domain for Trusted Cloud Computing\nJaewon Choi, Jongse Park, Jinho Seol, and Seungryoul Maeng\nCCGrid, 2013 [Paper]\nLocality-aware Dynamic VM Reconfiguration on MapReduce Clouds\nJongse Park, Daewoo Lee, Bokyeong Kim, Jaehyuk Huh, and Seungryoul Maeng\nHPDC, 2012 [Paper|Talk]\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"23d1e75f872528fc12f5f2b142375ff7","permalink":"https://jongse-park.github.io/publications/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/publications/","section":"","summary":"2025 Pimba: A Processing-in-Memory Acceleration for Post-Transformer Large Language Model Serving\nWonung Kim, Yubin Lee, Yoonsung Kim, Jinwoo Hwang, Seongryong Oh, Jiyong Jung, Aziz Huseynov, Woong Gyu Park, Chang Hyun Park, Divya Mahajan, Jongse Park\nMICRO, 2025 [Paper] (To Appear)\nPyTorchSim: A Comprehensive, Fast, and Accurate NPU Simulation Framework\nWonhyuk Yang, Yunseon Shin, Okkyun Woo, Geonwoo Park, Hyungkyu Ham, Jeehoon Kang, Jongse Park, Gwangsun Kim\nMICRO, 2025 [Paper] (To Appear)","tags":null,"title":"Publications","type":"page"},{"authors":[],"categories":[],"content":"We are looking for highly motivated undergraduate interns! Note: This internship position is for Prof. Jongse Park\u0026rsquo;s group within CASYS Lab. Note: This internship position can be registered as an individual study or a URP.\nResearch Topics HW/SW co-design for system software acceleration Acceleration for machine learning workloads (e.g., DNN training) DNN-based video analytics systems Processing-in-Memory (PIM) CXL-enabled accelerated systems Computing stack design for accelerated systems (e.g., language, compiler, runtime, and hardware) Eligibility We are looking for students who have ample programming experience in C/C++. We are looking for students who have taken [CS230] System Programming, [CS311] Computer Organization, and (preferably) [CS330] OS. Students who have taken artificial intelligence courses (e.g., CS376) are preferred. Students who have hardware development skills (e.g., Verilog) are preferred, but not required. We are looking for students who would like to commit for at least \u0026ldquo;TWO\u0026rdquo; semesters (e.g., Summer 2022 \u0026ndash; Winter 2022). What\u0026rsquo;s Expected Every student will work on a research project: (1) your own project, or (2) on-going project. You will lead or attend a weekly meeting for the research project. You will attend two weekly meet-ups of CASYS-Park group: (1) group meeting, and (2) paper reading session. Working Environment Designated desk in the lab Desktop and any number of monitors you need Month salary depending on your contribution How to Apply Fill out a online application form (Deadline: June 3rd): Google Form Link. Schedule an interview with Jongse Start working together! :) ","date":1652919176,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1652919176,"objectID":"03560ee2ab2cb73995a44e520b22dffd","permalink":"https://jongse-park.github.io/recruitment/","publishdate":"2022-05-19T09:12:56+09:00","relpermalink":"/recruitment/","section":"","summary":"We are looking for highly motivated undergraduate interns! Note: This internship position is for Prof. Jongse Park\u0026rsquo;s group within CASYS Lab. Note: This internship position can be registered as an individual study or a URP.\nResearch Topics HW/SW co-design for system software acceleration Acceleration for machine learning workloads (e.g., DNN training) DNN-based video analytics systems Processing-in-Memory (PIM) CXL-enabled accelerated systems Computing stack design for accelerated systems (e.g., language, compiler, runtime, and hardware) Eligibility We are looking for students who have ample programming experience in C/C++.","tags":[],"title":"CASYS-Park Internship Positions (Summer 2022 ~ )","type":"page"}]